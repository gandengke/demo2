package com.jd.easy.audience.task.plugin.step.audience;import com.jd.easy.audience.common.audience.AudienceSourceTypeEnum;import com.jd.easy.audience.common.audience.AudienceTaskStatus;import com.jd.easy.audience.common.constant.NumberConstant;import com.jd.easy.audience.common.exception.JobInterruptedException;import com.jd.easy.audience.common.oss.JdCloudDataConfig;import com.jd.easy.audience.common.oss.OssFileTypeEnum;import com.jd.easy.audience.common.util.JdCloudOssUtil;import com.jd.easy.audience.task.commonbean.bean.AudienceDataSourceBean;import com.jd.easy.audience.task.commonbean.bean.AudienceGenerateBean;import com.jd.easy.audience.task.driven.step.StepCommon;import com.jd.easy.audience.task.driven.step.StepCommonBean;import com.jd.easy.audience.task.generator.bean.Custom4AChainBean;import com.jd.easy.audience.task.generator.bean.Custom4ALayoutBean;import com.jd.easy.audience.task.generator.service.GenerateSQLService;import com.jd.easy.audience.task.generator.service.impl.GenerateSQLServiceImpl;import com.jd.easy.audience.task.generator.util.DateUtil;import com.jd.easy.audience.task.generator.util.GeneratorUtil;import com.jd.easy.audience.task.plugin.property.ConfigProperties;import com.jd.jss.JingdongStorageService;import com.typesafe.config.Config;import org.apache.commons.lang.StringUtils;import org.apache.hadoop.conf.Configuration;import org.apache.hadoop.fs.FileSystem;import org.apache.hadoop.fs.Path;import org.apache.spark.api.java.function.FilterFunction;import org.apache.spark.sql.Dataset;import org.apache.spark.sql.Row;import org.apache.spark.sql.SparkSession;import org.apache.spark.storage.StorageLevel;import org.slf4j.Logger;import org.slf4j.LoggerFactory;import java.io.File;import java.io.IOException;import java.text.MessageFormat;import java.util.HashMap;import java.util.Iterator;import java.util.List;import java.util.Map;import java.util.stream.Collectors;/** * @author cdxiongmei * @title AudienceGenerateStep * @description 根据人群定义、人群包临时表映射关系生成人群圈选SQL, 提交spark作业生成人群包。 * @date 2021/8/10 上午10:43 */public class AudienceGenerateStep extends StepCommon<Map<String, Object>> {    /**     * log工具     */    private static final Logger LOGGER = LoggerFactory.getLogger(AudienceGenerateStep.class);    @Override    public void validate() {        super.validate();        AudienceGenerateBean bean;        if (getStepBean() instanceof AudienceGenerateBean) {            bean = (AudienceGenerateBean) getStepBean();        } else {            throw new JobInterruptedException("bean 参数异常", "bean parameter occurs errors;");        }        if (StringUtils.isBlank(bean.getStepName())) {            throw new RuntimeException("stepName is null in AudienceGenerateStep");        }        if (StringUtils.isBlank(bean.getSparkSQL())) {            throw new RuntimeException("sparkSql is null in AudienceGenerateStep");        }    }    @Override    public Map<String, Object> run(Map<String, StepCommonBean> dependencies) {        //1 构建sparkSession        AudienceGenerateBean bean;        if (getStepBean() instanceof AudienceGenerateBean) {            bean = (AudienceGenerateBean) getStepBean();        } else {            throw new JobInterruptedException("AudienceGenerateBean 参数异常", "AudienceGenerateBean parameter occurs errors;");        }        SparkSession sparkSession = SparkSession.builder().appName(bean.getStepName()).enableHiveSupport().getOrCreate();        //1.1 注册udf函数//        sparkSession.udf().register("contains_status",//                (UDF4<WrappedArray<Integer>, Integer, Integer, WrappedArray<Integer>, Boolean>) SparkUdfUtil::containsStatus, DataTypes.BooleanType);        //1.2 返回值处理        Map<String, Object> outputMap = new HashMap<>(NumberConstant.INT_20);        bean.setOutputMap(outputMap);        outputMap.put(ConfigProperties.AUDIENCE_STATUS_KEY, AudienceTaskStatus.EXECUTE_QUERY.name());        sparkSession.sql("use " + bean.getDbName());        Map<Long, List<AudienceDataSourceBean>> sourceMap = bean.getDataSource().stream().collect(Collectors.groupingBy(AudienceDataSourceBean::getUserIdType));        Long sourceUserIdType = sourceMap.keySet().stream().findFirst().get();        if (bean.isUseOneId() && sourceMap.size() == NumberConstant.INT_1 && sourceUserIdType.equals(ConfigProperties.ONEID_UNKNOWNTYPE)) {            //数据源都是未知oneid用户类型，但需要生成其他结果，默认没有数据            throw new JobInterruptedException("配置中存在不支持oneid的用户标示类型", "Audience sources have some unsupport userid type ");        }        //2 sparksql：需要根据配置的标签筛选条件，以及人群交并差进行拼装        Dataset<Row> dataset = generateSql(sparkSession, bean, false);        //3 人群包大小校验        bean.setAudienceSize(dataset.count());        audienceSizeValidate(bean.getAudienceSize(), bean.getLimitSize());        return new HashMap<String, Object>(NumberConstant.INT_1) {            {                put("tmpData", dataset);            }        };    }    private Map<String, Dataset<Row>> generate4aTempTable(SparkSession sparkSession, AudienceDataSourceBean sourceInfo, Boolean isUseOneId, Dataset<Row> oneIdDs, Boolean isTest) {        Map<String, Dataset<Row>> result = new HashMap<>(NumberConstant.INT_20);        List<Object> beans = GeneratorUtil.get4aBeans().get(sourceInfo.getSourceId());        JingdongStorageService jfsClient = JdCloudOssUtil.createJfsClient(sourceInfo.getEndPoint(), sourceInfo.getAccessKey(), sourceInfo.getSecretKey());        for (Object tmpBean : beans) {            String jfsObjectPreKey = sourceInfo.getPackageUrl();            Dataset<Row> dataset = null;            //临时表名            String tmpTableName = "";            if (tmpBean instanceof Custom4ALayoutBean) {                // 4a分布筛选 需要把日期范围内的数据进行合并                Custom4ALayoutBean beanCur = ((Custom4ALayoutBean) tmpBean);                tmpTableName = beanCur.getTmpTableName();                if (isTest) {                    dataset = sparkSession.read().table(jfsObjectPreKey);                    if (isUseOneId && !sourceInfo.getUserIdType().equals(ConfigProperties.ONEID_ONEIDTYPE)) {                        //TODO 和oneid表进行关联,如果本身是oneid，则不用关联                        dataset = dataset.withColumnRenamed("user_id", "id").join(oneIdDs.filter("id_type=" + sourceInfo.getUserIdType()).selectExpr("id", "CAST(one_id AS STRING) AS user_id"), "id")                                .drop("id");                    }//                    dataset.persist(StorageLevel.MEMORY_AND_DISK_SER_2());                    dataset.createOrReplaceTempView(tmpTableName);                    result.put(tmpTableName, dataset);                    continue;                }                List<String> dateList = DateUtil.getDayList(DateUtil.addDays(beanCur.getStartDate(), NumberConstant.INT_1), beanCur.getEndDate());                for (String dateEle : dateList) {                    //获取每个日期的4a明细数据                    String uddStatusPath = JdCloudOssUtil.getUddStatusDataKey(jfsObjectPreKey, dateEle);                    if (!JdCloudOssUtil.objectExist(jfsClient, sourceInfo.getBucketName(), uddStatusPath)) {                        LOGGER.info("4aassets key not exsit:{}", uddStatusPath);                        continue;                    }                    JdCloudDataConfig readConfig = new JdCloudDataConfig(sourceInfo.getEndPoint(), sourceInfo.getAccessKey(), sourceInfo.getSecretKey());                    readConfig.setOptions(new HashMap<String, String>() {                        {                            put("header", "true");                        }                    });                    readConfig.setClientType(ConfigProperties.getClientType());                    readConfig.setFileType(OssFileTypeEnum.parquet);                    readConfig.setObjectKey(sourceInfo.getBucketName() + File.separator + uddStatusPath);                    Dataset<Row> datasetNew = JdCloudOssUtil.readDatasetFromOss(sparkSession, readConfig);                    dataset = (dataset == null) ? datasetNew : dataset.union(datasetNew);                }            } else if (tmpBean instanceof Custom4AChainBean) {                //4a流转筛选                Custom4AChainBean beanCur = ((Custom4AChainBean) tmpBean);                tmpTableName = beanCur.getTmpTableName();                if (isTest) {                    dataset = sparkSession.read().table(jfsObjectPreKey);                    if (isUseOneId && !sourceInfo.getUserIdType().equals(ConfigProperties.ONEID_ONEIDTYPE)) {                        //TODO 和oneid表进行关联,如果本身是oneid，则不用关联                        dataset = dataset.withColumnRenamed("user_id", "id").join(oneIdDs.filter("id_type=" + sourceInfo.getUserIdType()).selectExpr("id", "CAST(one_id AS STRING) AS user_id"), "id")                                .drop("id");                    }                    dataset = sparkSession.read().table(jfsObjectPreKey);                    dataset.persist(StorageLevel.MEMORY_AND_DISK_SER_2());                    dataset.createOrReplaceTempView(tmpTableName);                    result.put(tmpTableName, dataset);                    continue;                }                String uddStatusPath = JdCloudOssUtil.getUddStatusDataKey(jfsObjectPreKey, beanCur.getStartDate());                String uddStatusEndPath = JdCloudOssUtil.getUddStatusDataKey(jfsObjectPreKey, beanCur.getEndDate());                if (!JdCloudOssUtil.objectExist(jfsClient, sourceInfo.getBucketName(), uddStatusPath)) {                    LOGGER.info("4achain key not exsit:{}", uddStatusPath);                } else {                    JdCloudDataConfig readConfig = new JdCloudDataConfig(sourceInfo.getEndPoint(), sourceInfo.getAccessKey(), sourceInfo.getSecretKey());                    readConfig.setOptions(new HashMap<String, String>() {                        {                            put("header", "true");                        }                    });                    readConfig.setClientType(ConfigProperties.getClientType());                    readConfig.setFileType(OssFileTypeEnum.parquet);                    readConfig.setObjectKey(sourceInfo.getBucketName() + File.separator + uddStatusPath);                    Dataset<Row> datasetStart = JdCloudOssUtil.readDatasetFromOss(sparkSession, readConfig);                    dataset = (dataset == null) ? datasetStart : dataset.union(datasetStart);                }                if (!JdCloudOssUtil.objectExist(jfsClient, sourceInfo.getBucketName(), uddStatusEndPath)) {                    LOGGER.info("4achain key not exsit:{}", uddStatusEndPath);                } else {                    JdCloudDataConfig readConfig = new JdCloudDataConfig(sourceInfo.getEndPoint(), sourceInfo.getAccessKey(), sourceInfo.getSecretKey());                    readConfig.setOptions(new HashMap<String, String>() {                        {                            put("header", "true");                        }                    });                    readConfig.setClientType(ConfigProperties.getClientType());                    readConfig.setFileType(OssFileTypeEnum.parquet);                    readConfig.setObjectKey(sourceInfo.getBucketName() + File.separator + uddStatusEndPath);                    Dataset<Row> datasetEnd = JdCloudOssUtil.readDatasetFromOss(sparkSession, readConfig);                    dataset = (dataset == null) ? datasetEnd : dataset.union(datasetEnd);                }            }            if (null == dataset) {                throw new JobInterruptedException("4a数据获取失败", "4a data failed!");            }            if (isUseOneId && !sourceInfo.getUserIdType().equals(ConfigProperties.ONEID_ONEIDTYPE)) {                //TODO 和oneid表进行关联,如果本身是oneid，则不用关联                dataset = dataset.withColumnRenamed("user_id", "id").join(oneIdDs.filter("id_type=" + sourceInfo.getUserIdType()).selectExpr("id", "CAST(one_id AS STRING) AS user_id"), "id")                        .drop("id");            }            dataset.persist(StorageLevel.MEMORY_AND_DISK_SER_2());            dataset.createOrReplaceTempView(tmpTableName);            result.put(tmpTableName, dataset);        }        return result;    }    /**     * @param sparkSession spark     * @param bean         参数定义     * @description 根据已有人群包或者标签数据集生成临时表     * @date 2022/1/26 上午10:17     */    private void generateTempTable(SparkSession sparkSession, AudienceGenerateBean bean, Dataset<Row> oneIdDs, Boolean isTest) {        List<AudienceDataSourceBean> sourceList = bean.getDataSource();        final Iterator<AudienceDataSourceBean> iterator = sourceList.iterator();        Map<String, Dataset<Row>> result = new HashMap<>(NumberConstant.INT_20);        while (iterator.hasNext()) {            final AudienceDataSourceBean sourceInfo = iterator.next();            Dataset<Row> dataset = null;            //临时表名            String tmpTableName = "";            if (sourceInfo.getSourceType().getValue().equals(AudienceSourceTypeEnum.layout4a.getValue()) ||                    sourceInfo.getSourceType().getValue().equals(AudienceSourceTypeEnum.chain4a.getValue())) {                LOGGER.info("使用4a圈选人群");                //如果是4a分布或者4a流转                result.putAll(generate4aTempTable(sparkSession, sourceInfo, bean.isUseOneId(), oneIdDs, isTest));                continue;            } else if (AudienceSourceTypeEnum.commonlabel.getValue().equals(sourceInfo.getSourceType().getValue())) {                Map<String, String> viewsMap = GeneratorUtil.getCommonLabelView();                LOGGER.info("使用统一标签模型圈选人群{}", viewsMap);                tmpTableName = viewsMap.get(sourceInfo.getSourceId());                String labelTable = sourceInfo.getPackageUrl();                LOGGER.info("tableName:{}", tmpTableName);                Config templateConfig = ConfigProperties.getsqlTemplate(this.getClass().getSimpleName(), new Exception().getStackTrace()[NumberConstant.INT_0].getMethodName());                String filterSql = MessageFormat.format(templateConfig.getString("commonLabelSearch"), labelTable, sourceInfo.getSourceId());//                String filterSql = "SELECT\n" +//                        "\t*\n" +//                        "FROM\n" +//                        "\t" + labelTable + " WHERE set_id=\'" + sourceInfo.getSourceId() + "\'\n";                LOGGER.info("filterSQL:" + filterSql);                dataset = sparkSession.sql(filterSql);//                dataset.persist(StorageLevel.MEMORY_AND_DISK_SER_2());//                dataset.createOrReplaceTempView(tmpTableName);//                result.put(tmpTableName, dataset);                LOGGER.info("created TempView {}->{}", sourceInfo.getSourceId(), tmpTableName);            } else if (AudienceSourceTypeEnum.rfmlevel.equals(sourceInfo.getSourceType()) || AudienceSourceTypeEnum.rfmaction.equals(sourceInfo.getSourceType())) {                //rfm数据                LOGGER.info("使用rfm圈选人群");                tmpTableName = AudienceSourceTypeEnum.getTempTableName(sourceInfo.getSourceId(), sourceInfo.getSourceType());                String userObjectKey = JdCloudOssUtil.getRfmUserDetDataKey(sourceInfo.getPackageUrl());                JdCloudDataConfig readConfig = new JdCloudDataConfig(sourceInfo.getEndPoint(), sourceInfo.getAccessKey(), sourceInfo.getSecretKey());                readConfig.setOptions(new HashMap<String, String>() {                    {                        put("header", "true");                    }                });                readConfig.setClientType(ConfigProperties.getClientType());                readConfig.setFileType(OssFileTypeEnum.parquet);                readConfig.setObjectKey(sourceInfo.getBucketName() + File.separator + userObjectKey);                Dataset<Row> userDet = JdCloudOssUtil.readDatasetFromOss(sparkSession, readConfig);                LOGGER.info("ready write temp view :{}", tmpTableName);                userDet.createOrReplaceTempView(tmpTableName);            }//            else if (AudienceSourceTypeEnum.audiencepackage.equals(sourceInfo.getSourceType())) {//                //人群包兼容历史，不实用目录直接读取//                /**//                 * 保存人群包到本地//                 *///                tmpTableName = AudienceSourceTypeEnum.getTempTableName(sourceInfo.getSourceId(), sourceInfo.getSourceType());//                String hdfsPath = "audience_data/tmp/" + sourceInfo.getSourceId();//                JingdongStorageService jfsClient = JdCloudOssUtil.createJfsClient(sourceInfo.getEndPoint(), sourceInfo.getAccessKey(), sourceInfo.getSecretKey());//                String hdfsFilePath = JdCloudOssUtil.copyOssFile2Hdfs(jfsClient, sparkSession.sparkContext().hadoopConfiguration(), sourceInfo.getBucketName(), sourceInfo.getPackageUrl(), hdfsPath);//                dataset = sparkSession.read().option("header", "true").format("csv").load(hdfsFilePath);//            }            else {                tmpTableName = AudienceSourceTypeEnum.getTempTableName(sourceInfo.getSourceId(), sourceInfo.getSourceType());                if (isTest) {                    dataset = sparkSession.read().table(sourceInfo.getPackageUrl());                } else {                    JdCloudDataConfig readConfig = new JdCloudDataConfig(sourceInfo.getEndPoint(), sourceInfo.getAccessKey(), sourceInfo.getSecretKey());                    readConfig.setOptions(new HashMap<String, String>() {                        {                            put("header", "true");                            put("inferschema", "true");                        }                    });                    readConfig.setClientType(ConfigProperties.getClientType());                    readConfig.setFileType(OssFileTypeEnum.csv);                    readConfig.setObjectKey(sourceInfo.getBucketName() + File.separator + sourceInfo.getPackageUrl());                    dataset = JdCloudOssUtil.readDatasetFromOss(sparkSession, readConfig);                }            }            if (bean.isUseOneId() && !sourceInfo.getUserIdType().equals(ConfigProperties.ONEID_ONEIDTYPE)) {                //和oneid表进行关联,如果本身是oneid，则不用关联                dataset = dataset.withColumnRenamed("user_id", "id").join(oneIdDs.filter("id_type=" + sourceInfo.getUserIdType()).selectExpr("id", "CAST(one_id AS STRING) AS user_id"), "id")                        .drop("id");            }            dataset.persist(StorageLevel.MEMORY_AND_DISK_SER_2());            dataset.printSchema();            dataset.createOrReplaceTempView(tmpTableName);            result.put(tmpTableName, dataset);        }        LOGGER.info("generateTempTable result = {}", result.toString());    }    /**     * @param sparkSession     * @param bean         参数定义     * @return org.apache.spark.sql.Dataset<org.apache.spark.sql.Row>     * @description 生成查询语句     * @date 2022/1/26 上午10:23     */    public Dataset<Row> generateSql(SparkSession sparkSession, AudienceGenerateBean bean, Boolean isTest) {        //0.获取oneid表数据        LOGGER.info("读取oneid数据");        Dataset<Row> oneIdInfoView = null;        if (bean.isUseOneId()) {            oneIdInfoView = sparkSession.read().table(ConfigProperties.ONEID_DETAIL_TB);            oneIdInfoView.cache();        }        //1. 模版引擎生成sql        GenerateSQLService generateSqlService = new GenerateSQLServiceImpl();        String sql = generateSqlService.generateSparkSQL(bean.getSparkSQL());        LOGGER.error("sparkSql:{}", sql);        //2. 生成临时表        generateTempTable(sparkSession, bean, oneIdInfoView, isTest);        //3. 查询数据        Dataset<Row> retDataset = sparkSession.sql(sql)                .filter((FilterFunction<Row>) row -> StringUtils.isNotBlank(row.get(NumberConstant.INT_0).toString()));//        retDataset.persist(StorageLevel.MEMORY_AND_DISK_SER_2());        audienceSizeValidate(retDataset.count(), bean.getLimitSize());        /**         * 对于生成结果需要进行oneid关联         */        if (bean.isUseOneId()) {            //TODO 和oneid表进行关联,如果本身是oneid，则不用关联            retDataset = retDataset.withColumnRenamed("user_id", "one_id").join(oneIdInfoView.filter("id_type=" + bean.getUserIdType()), "one_id").selectExpr("id AS user_id");        }        return retDataset;    }    /**     * @title audienceSizeValidate     * @description 人群包下限限制     * @param: audienceCnt     * @param: limitSize     */    public void audienceSizeValidate(Long audienceCnt, Long limitSize) {        if (audienceCnt < limitSize) {            throw new JobInterruptedException("人群包大小小于" + limitSize, "Audience package size less than " + limitSize);        }    }    private void cleanHdfsFile(String filePath, Configuration conf) {        if (filePath == null || conf == null) {            return;        }        FileSystem fileSystem = null;        try {            fileSystem = FileSystem.get(conf);            fileSystem.delete(new Path(filePath), true);        } catch (IOException e) {            System.out.println("delete the file error:" + e.getMessage());        }    }    @Override    public String toString() {        AudienceGenerateBean bean;        if (getStepBean() instanceof AudienceGenerateBean) {            bean = (AudienceGenerateBean) getStepBean();        } else {            throw new JobInterruptedException("bean 参数异常", "bean parameter occurs errors;");        }        return "AudienceGenerateStep{" +                "sparkSQL='" + bean.getSparkSQL() + '\'' +                ", audienceId='" + bean.getAudienceId() + '\'' +                ", audienceSize=" + bean.getAudienceSize() +                ", dataSource=" + bean.getDataSource() +                '}';    }}